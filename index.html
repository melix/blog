<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Cédric Champeau's blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="keywords" content="">    

    <!-- Le styles -->
    <link href="/blog/css/bootstrap.min.css" rel="stylesheet">
    <link href="/blog/css/asciidoctor.css" rel="stylesheet">
    <link href="/blog/css/base.css" rel="stylesheet">
   <script src="https://kit.fontawesome.com/fefa3ec5bf.js" crossorigin="anonymous"></script>
      
    <script src="/blog/js/highlight.min.js"></script>
    <link href="/blog/css/equilibrium-gray-light.min.css" rel="stylesheet">
    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="js/html5shiv.min.js"></script>
    <![endif]-->

    <!-- Fav and touch icons -->
    <!--<link rel="apple-touch-icon-precomposed" sizes="144x144" href="../assets/ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../assets/ico/apple-touch-icon-114-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../assets/ico/apple-touch-icon-72-precomposed.png">
    <link rel="apple-touch-icon-precomposed" href="../assets/ico/apple-touch-icon-57-precomposed.png">-->
    <link rel="shortcut icon" href="favicon.ico">
  </head>
  <body>
    <div id="wrap">
   
	
	<!-- Fixed navbar -->
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
          <div class="container">
	    <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/blog">Cédric Champeau's blog</a>
	    </div>
            <div class="navbar-collapse collapse">
              <ul class="nav navbar-nav">                
                <li><a href="/blog/about.html">About</a></li>
		<li><a href="/blog/projects.html">Projects</a></li>
		<li><a href="/blog/astronomy.html">Astronomy</a></li>
		<li class="dropdown">
          		<a data-toggle="dropdown" href="#">Topics<b class="caret"></b></a>
		          <ul class="dropdown-menu" role="menu">
				<li><a href="/blog/tags/gradle.html">Gradle</a></li>
                <li><a href="/blog/tags/groovy.html">Groovy</a></li>
                <li><a href="/blog/tags/micronaut.html">Micronaut</a></li>
                <li><a href="/blog/tags/index.html">See all tags</a></li>
		          </ul>
	       </li>
                <li><a href="/blog/feed.xml">Feed</a></li>                
              </ul>
	      <ul class="nav navbar-nav navbar-right">
	      	<li><a href="https://github.com/melix"><i class="fa fa-github"></i></a></li>
	        <li><a href="https://twitter.com/CedricChampeau"><i class="fa fa-twitter"></i></a></li>
	        <li><a rel="me" href="https://mastodon.xyz/@melix"><i class="fa-brands fa-mastodon"></i></a></li>
	        <li><a rel="me" href="https://astrodon.social/@melix"><i class="fa-brands fa-mastodon"></i></a></li>
	      </ul>
            </div>
          </div>
      </div>
      <div class="container">

	<div class="page-header">
		<h1>Blog</h1>
	</div>
  			<a href="/blog/2024/01-04-jsolex-2.0.html"><h1>Stacking and mosaic creation with JSol&#8217;Ex 2.0</h1></a>
  			<p>04 January 2024</p>
			<p><em>Tags: </em>
		<a href="/blog/tags/astronomy.html">astronomy</a> 
	
		<a href="/blog/tags/astro4j.html">astro4j</a> 
	
		<a href="/blog/tags/solex.html">solex</a> 
	
		<a href="/blog/tags/java.html">java</a> 
	</p>
			
  			<p><div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>A couple days ago I have released <a href="https://github.com/melix/astro4j/releases/tag/2.0.0">JSol&#8217;Ex 2.0</a>.
This software can be used as an alternative to <a href="http://valerie.desnoux.free.fr/inti/">INTI</a> to process solar images acquired using Christian Buil&#8217;s <a href="http://www.astrosurf.com/solex/sol-ex-presentation-en.html">Sol&#8217;Ex</a>.
This new version introduces 2 new features that I would like to describe in more details in this blog post: stacking and mosaic stitching.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_stacking">Stacking</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Stacking should be something familiar to anyone doing planetary imaging.
One of the most popular sofware for doing stacking is <a href="https://www.autostakkert.com/wp/">AutoStakkert!</a> which has recently seen a new major version.
Stacking usually consists of taking a large number of images, selecting a few reference points and trying to align these images to reconstruct a single, stacked image which increases the signal-to-noise ratio.
Each of the individual images are usually small and there are a large number of images (since the goal is to reduce the impact of turbulence, typically, videos are taken with a high frame rate, often higher than 100 frames per second).
In addition, there are little to no changes between the details of a series (granted that you limit the capture to a few seconds, to avoid the rotation of the planet typically) so the images are really "only" disformed by turbulence.
In the context of Sol&#8217;Ex image processing, the situation is a bit different: we have a few captures of large images: in practice, capturing an image takes time (it&#8217;s a scan of the sun which will consist of a video of several seconds just to build a <em>single</em> image) and the solar details can move quickly between captures.
In practice, it means that you can reasonably stack 2 to 5 images, maybe more if the scans are quick enough and that there are not too many changes between scans.</p>
</div>
<div class="paragraph">
<p>The question for me was how to implement such an algorithm for JSol&#8217;Ex?
Compared to planetary image stacking, we have a few advantages:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>images have a great resolution: depending on the camera that you use and the binning mode, you can have images which range from several hundreds pixels large to a few thousands pixels</p>
</li>
<li>
<p>the images are well defined : in planetary observation, there are a few "high quality" images in a video of several thousand frames, but most images are either fully or partially disformed</p>
</li>
<li>
<p>there&#8217;s little movement between images: anyone who has stacked a planetary video can see that it&#8217;s frequent to see jumps of several pixels between 2 images, just because of turbulence</p>
</li>
<li>
<p>for each image, we already have determined the ellipse which makes the solar disk and should also have corrected the geometry, so that all solar disks are "perfect circles"</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Therefore, a naive approach, which I tried without success a few months ago, is a geometric approach where we simply make all solar disks the same (by resizing), align them then create an average image.
To illustrate this approach, let&#8217;s look at some images:</p>
</div>
 <video width="1024" height="768" controls>
  <source src="https://melix.github.io/blog/img/jsolex2/reference.webm" type="video/webm">
Your browser does not support the video tag.
</video>
<div class="paragraph">
<p>In this video we can see that there is quite some movement visible between each image.
Each of them is already of quite good quality, but we can notice some noise and more importantly, a shear effect due to the fact that a scan takes several seconds and that we reconstruct line by line :</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/jsolex2/stack-ref.jpg" alt="stack ref">
</div>
</div>
<div class="paragraph">
<p>The average image is therefore quite blurry:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/jsolex2/stack-average.jpg" alt="stack average">
</div>
</div>
<div class="paragraph">
<p>Therefore, using the average image is <em>not</em> a good option for stacking and that&#8217;s why I recommended to use <a href="https://www.autostakkert.com/wp/">AutoStakkert!</a> instead, which gave better results.</p>
</div>
<div class="paragraph">
<p>In order to achieve better results, I opted for a simple yet effective algorithm:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>first, estimate the sharpness of each image. This is done by computing the <a href="https://en.wikipedia.org/wiki/Laplace_operator">Laplacian</a> of each image. The image with the best sharpness is selected as the <em>reference</em> image</p>
</li>
<li>
<p>divide each image into tiles (by default, a tile has a width of 32 pixels)</p>
</li>
<li>
<p>for each tile, try to align it with the reference image by computing an error between the reference tile and the image</p>
</li>
<li>
<p>the error is based on root mean squared error of the pixel intensities : the better the tiles are aligned, the closer to 0 the error will be</p>
</li>
<li>
<p>this is the most expensive operation, because it requires computing the error for various positions</p>
</li>
<li>
<p>we&#8217;re only looking for displacements with a maximum shift of 2/3 of the tile size (so, by default, 21 pixels maximum)</p>
</li>
<li>
<p>if the shift between 2 tiles is higher than this limit, we won&#8217;t be able to align the tiles properly</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>We could have stopped here and already reconstruct an image at this stage, but the result wouldn&#8217;t be great: the fact that we use tiles would be visible at the edges of the tiles, with square artifacts clearly visible.
To reduce the artifacts, I opted for a "sliding window" algorith, where the next tile will overlap the previous one by a factor between 0 (no overlap) and 1 (100% overlap).
This means that for each pixel, we will get a "stack" of pixel values computed from the alignment of several tiles.
The final pixel value is then computed by taking the median value of the stack.
Even so, some stacking vertical or horizontal artifacts can still be sometimes visible, so the last "trick" I used is to build the stacks by only taking pixels within a certain radius, instead of the whole square.</p>
</div>
<div class="paragraph">
<p>The resulting, stacked image is here:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/jsolex2/stack-jsolex.jpg" alt="stack jsolex">
</div>
</div>
<div class="paragraph">
<p>We can see that:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>noise from the original images is gone</p>
</li>
<li>
<p>shearing artifacts are significantly reduced</p>
</li>
<li>
<p>the resulting image is not as blurry as the average version</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>There were, however, some compromises I had to make, in order to avoid that the stacking process takes too long.
In particular, the tile alignment process (in particular error computation) is very expensive, since for each tile, we have to compute 21*21 = 441 errors by default.
With an overlap factor of 0.3, that&#8217;s, for an image of 1024 pixels large, more than 5 million errors to compute.
Even computing them in parallel takes long, therefore I added <a href="https://en.wikipedia.org/wiki/Local_search_(optimization)">local search optimization</a>: basically, instead of searching in the whole space, I&#8217;m only looking for errors within a restricted radius (8 pixels).
Then, we take the minimal error of this area and resume searching from that position: step by step we&#8217;re moving "closer" to a local optimum which will hopefully be the best possible error.
While this doesn&#8217;t guarantee to find the best possible solution, it proved to provide very good results while significantly cutting down the computation times.</p>
</div>
<div class="paragraph">
<p>From several tests I made, the quality of the stacked image matches that of Autostakkert!.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mosaic_composition">Mosaic composition</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The next feature I added in JSol&#8217;Ex 2, which is also the one which took me most time to implement, is mosaic composition.
To some extent, this feature is similar to stacking, except that in stacking, we know that all images represent the same region of the solar disk and that they are roughly aligned.
With mosaics, we have to work with different regions of the solar disk which overlap, and need to be stitched together in order to compose a larger image.</p>
</div>
<div class="paragraph">
<p>On December 7th, 2024, I had given a glimpse of that feature for <a href="https://www.astro-images-processing.fr/articles/135166-pratique-et-traitement-d-images-sol-ex">the french astrophotograhers association AIP</a>, but I wasn&#8217;t happy enough with the result so decided to delay the release.
Even today, I&#8217;m not fully satisfied, but it gives reasonable results on several images I tried so decided it was good enough for public release and getting feedback about this feature.</p>
</div>
<div class="paragraph">
<p>Mosaic composition is not an easy task: there are several problems we have to solve:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>first, we need to identify, in each image, the regions which "overlap"</p>
</li>
<li>
<p>then for each image, we need to be able to tell if the pixel value we read at a particular place is relevant for the whole composition or not</p>
</li>
<li>
<p>then we have to do the alignment</p>
</li>
<li>
<p>and finally avoid mosaicing artifacts, typically vertical or horizontal lines at the "edges"</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In addition, mosaic composition is not immune to the problem that each image can have different illumination, or even that the regions which are overlapping have slightly (or even sometimes significantly) moved between the captures.
Therefore, the idea is to "warp" images together in order to make them stitch smoothly.</p>
</div>
<div class="sect2">
<h3 id="_preparing_panels_for_integration">Preparing panels for integration</h3>
<div class="paragraph">
<p>Here are the main steps of the algorithm I have implemented:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>resize images so that they all solar disks have the same radius (in pixels), and that all images are square</p>
</li>
<li>
<p>normalize the histograms of each panel so that all images have similar lightness</p>
</li>
<li>
<p>estimate the background level of each panel, in order to have a good estimate of when a pixel of an image is relevant or not and perform background neutralization</p>
</li>
<li>
<p>there can be more than 2 panels to integrate. My algorithm works by stitching them 2 by 2, which implies sorting the panels by putting the panels which overlap the most in front, then stitching the 2 most overlapping panels together. The result of the operation is then stitched together with the next panel, until we have integrated all of them.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>The stitching part works quite differently than with typical stacking.
In stacking, we have <em>complete</em> data for each image: we "only" have to align them.
With mosaics, there are "missing" parts in the image that we need to fill in.
To do this, we have to identify which part of a panel can be blended into the reconstructed image in order to complete it.
This means that the alignment process is significanly more complicated than with typical stacking, since we will work on "missing" data.
Part of the difficulty is precisely identifying if something is missing or not, that is to say if the signal of a pixel in one of the panels is relevant to the composition of the final image.
This is done by comparing it with the estimated background level, but that&#8217;s not the only trick.</p>
</div>
<div class="paragraph">
<p>Despite the fact that our panels are supposedly aligned and that the circles representing the solar disks are supposed to be the same, in practice, depending on the quality of the capture and the ellipse regression success, the disks may be <em>slightly off</em>, with deformations.
There can even be slight rotations between panels (because of flexions at capture time, or processing artifacts).
As a consequence, a naive approach consisting of trying to minimize the error between 2 panels by moving them a few pixels in each direction like in stacking <em>doesn&#8217;t work</em>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>first of all, while you may properly align one edge of the solar disk, we can see that some regions will be misaligned. If these regions correspond to high contrast areas like filaments, it gives real bad results. If it happens at the edges of the sun, you can even see part of the disk being shifted a few pixels away from the other panel, which is clearly wrong.</p>
</li>
<li>
<p>second, estimating the error is not so simple, since we have <em>incomplete</em> disks. And in this case, the error has to be computed on large areas, which means that the operation is very expensive.</p>
</li>
<li>
<p>third, because we have to decide whether to pick a pixel from one panel or the other, this has the tendency to create very strong artifacts (vertical or horizontal lines) at the stitching edges</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_the_stitching_algorithm">The stitching algorithm</h3>
<div class="paragraph">
<p>Giving all the issues I described above, I chose to implement an algorithm which would work similarly to stacking, by "warping" a panel into another.
This process is iterative, and the idea is to take a "reference" panel, which is the one which has the most "relevant" pixels, and align tiles from the 2d panel into this reference panel.</p>
</div>
<div class="paragraph">
<p>To do this, we compute a grid of "reference points" which are in the "overlapping" area.
These points belong to the reference image, and one difficulty is to filter out points which belong to "incomplete" tiles.
Once we have these points, for each of them, we compute an alignment between the reference tile and the tile of the panel we&#8217;re trying to integrate.
This gives us, roughly, a "model" of how tiles are displaced in the overlapping area.
The larger the overlapping area is, the better the model will be, but experience shows that distorsion on one edge of the solar disk can be significanly different at the other edge.</p>
</div>
<div class="paragraph">
<p>The next step consists of trying to align tiles of the panel we integrate to the reference panel using this model.
This is where the iteration process happens.
In a nutshell, we have an area where the solar disk is "truncated".
Even if we split the image in tiles like with stacking, we cannot really tell whether a tile is "complete" or not, because it depends both on the pixel intensities of the reference panel and the second panel, and the background level.
In particular, calcium images may have dark areas <em>within</em> the solar disk which are sometimes as dark as the background.</p>
</div>
<div class="paragraph">
<p>If you are struggling to understand how difficult it can be to determine if part of the image we consider is relevant or not, let&#8217;s illustrate with this image:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/jsolex2/panel_noise.jpg" alt="panel noise">
</div>
</div>
<div class="paragraph">
<p>Can you see what&#8217;s wrong in this image?
Let&#8217;s increase constrast to make it clearly visible:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/jsolex2/panel_noise2.jpg" alt="panel noise2">
</div>
</div>
<div class="paragraph">
<p>Now it should be pretty obvious that below the south edge of the truncated disk, we have an area which has pixels which are above the value of the background, but do not constitute actual signal!
This problem took me quite some time to solve, and it&#8217;s only recently that I figured out a solution: before mosaicing, I am performing a background neutralization step, by modeling the background and substracting it from the image.
While this doesn&#8217;t fully solve the problem, it makes it much less relevant for composition.</p>
</div>
<div class="paragraph">
<p>In addition, we have to compose the image using tiles which are incomplete, and we don&#8217;t know the orientation of the panels: they can be assembled north/south, or west/east, and nothing tells us.
Potentially, it can even be a combination of these for a large number of panels.</p>
</div>
<div class="paragraph">
<p>Therefore, the algorithm works by creating a "mask" of the image being reconstructed.
This mask tells us "for this particular pixel, I have reconstructed a value, or the value of the reference image is good enough and we won&#8217;t touch it".
Then, for each tile, we consider tiles for which the mask is incomplete.</p>
</div>
<div class="paragraph">
<p>In order to determine how to align the truncated disk with data from the other image, we compute an estimate of the distortion of the tile based on the displacements models we have determined earlier.
Basically, for a new "tile" to be integrated, we will consider the sample "reference points" which are within a reasonable distance of the tile.
For this set of reference points, we know that they are "close enough" to compute an average model of the distorsion, that I call the "local distorsion": we can estimate, based on the distance of each reference point, how much they contribute to the final distorsion model for that particular point.</p>
</div>
<div class="paragraph">
<p>The key is really to consider <em>enough</em> samples to have a good distorsion model, but not too many because then the "locality" of alignment would become too problematic and we&#8217;d face misalignments.
Because there are not so many samples for each "incomplete" tile, we are in fact going to reconstruct, naturally, the image from the edges where there&#8217;s missing data: when there are no samples, it basically means we cannot compute a model, so we don&#8217;t know how to align tiles.
If we have enough samples, then we can compute a reliable model of the distorsion, and then we can reconstruct the missing part of each tile, by properly aligning the tiles together.
If the number of samples is not sufficient to consider a good model, then we assume that no distorsion happens, which is often the case for "background" tiles.</p>
</div>
<div class="paragraph">
<p>Most of the difficulty in this algorithm is properly identifying "when" we can stitch tiles together, that is to say when we can tell that the alignment between tiles makes sense and that the alignment is correct.
Often, I got good results for one kind of images (e.g, h-alpha images) but horrible results with others (e.g calcium) or the other way around.
I cannot really say I took a very scientific approach to this problem, but more an empirical approach, tweaking parameters of my algorithm until it gave good enough results in all cases.</p>
</div>
<div class="paragraph">
<p>I mentioned that the algorithm is iterative, but didn&#8217;t explain why yet: when we compute the tile alignments, we only do so because we have enough local samples for alignment.
We do this for all tiles that match this criteria, but we won&#8217;t, for example, be able to align a tile which is in the top of the image, if the bottom hasn&#8217;t been reconstructed.
Therefore, the iteration happens when we have reconstructed all the tiles we could in one step: then we recompute new reference points, and complete the image, not forgetting, of course, to update our mask to tell that some pixels were completed.</p>
</div>
<div class="paragraph">
<p>Overall the algorithm is fairly fast, and can be stopped once we have completed all tiles, or after a number of fixed iterations in case of difficulties (often due to the background itself).</p>
</div>
</div>
<div class="sect2">
<h3 id="_one_last_step">One last step</h3>
<div class="paragraph">
<p>The algorithm we&#8217;ve described provides us with a way to "roughly" reconstruct an image, but it doesn&#8217;t work like what you&#8217;d intuititvely think of mosaic composition, by "moving" 2 panels until they properly align and blend them toghether.
Instead, it will reconstruct an image by assembling <em>tiles</em> together, from what is <em>already</em> reconstructed: it is more fine grained, which will fix a number of the issues we&#8217;ve faced before: local distorsions, or images which are not properly aligned because the details at the surface at the sun <em>have moved</em> between the moment the first panel was captured and the second one did.</p>
</div>
<div class="paragraph">
<p>If we stopped there, we would see an image which looks like this:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/jsolex2/mosaic_reconstructed.jpg" alt="mosaic reconstructed">
</div>
</div>
<div class="paragraph">
<p>We can see a clear horizontal line, which is due to the fact that we&#8217;re reconstructing using tiles, and that depending on the alignment of tiles with the "missing" areas, we can have strong or weak artifacts at the borders.
Errors are even more visible in this image in calcium K line:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/jsolex2/mosaic_error_calcium.jpg" alt="mosaic error calcium">
</div>
</div>
<div class="paragraph">
<p>This time it&#8217;s very problematic and we are facing several of the issues we attempted to avoid: details have significantly moved between the north and south panel were captured, which leads to "shadowing" artifacts, and there are also tiling artifacts visible.</p>
</div>
<div class="paragraph">
<p>However, the image we get is <em>good enough</em> to perform one last step: use it as a <em>reference image</em> in the stacking algorithm we described in the first section of this blog post.
The reason stacking works well is because we know we have complete images that we can align.
Here, we have roughly reconstructed an image that we can use as a "complete reference".
The idea is therefore to take each tile of each panel and "blend" it using the reconstructed reference.
Of course, there is one big difference between the stacking in the first section and the stacking we have to do now.
We&#8217;re not really going to use the reconstructed image, except for aligning tiles together and computing a "weight" for each tile, which depends on the relative luminosity between the reference image tile we&#8217;re considering and the corresponding panel tile.</p>
</div>
<div class="paragraph">
<p>This gives us a pretty good result:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/jsolex2/mosaic_halpha_final.jpg" alt="mosaic halpha final">
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/jsolex2/mosaic_calcium_final.jpg" alt="mosaic calcium final">
</div>
</div>
<div class="paragraph">
<p>The images we got there are not perfect, which is why I&#8217;m not fully satisfied yet, but they are however already quite good, given that it&#8217;s all done in a few seconds, using the same software that you&#8217;d use to reconstruct Sol&#8217;Ex images!
In other words, the goal of these features is <em>not</em> to get the same level of quality that you&#8217;d get by using your favorite post-processing or mosaic composition software, but good enough to get you a reasonable result in a reasonable amount of time.</p>
</div>
<div class="paragraph">
<p>For example, on my machine, it takes less than one minute to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>stack images of the north and south panels (~10 images to stack)</p>
</li>
<li>
<p>stitch them together in a mosaic</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>It would have taken several minutes, or even more, using external software, especially for the mosaic part.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_conclusion">Conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this blog post, I have described how I got to implement 2 algorithms, the stacking algorithm and the mosaic composition one, in JSol&#8217;Ex.
None of the algorithms were based on any research paper: they were really designed in an "adhoc" way, as my intuition of how things could work.
It proved to be quite difficult, and it is very likely that better algorithms are described in the wild: I will consider them for future versions.</p>
</div>
<div class="paragraph">
<p>Nevertheless, I&#8217;m quite happy with the outcome, since, remember, I have started this program as an experiment and for learning purposes only.
Now I sincerely hope that it will help you get amazing solar images!</p>
</div>
</div>
</div></p>
  			<a href="/blog/2023/04-22-introducing-astro4j.html"><h1>Introducing astro4j</h1></a>
  			<p>22 April 2023</p>
			<p><em>Tags: </em>
		<a href="/blog/tags/astronomy.html">astronomy</a> 
	
		<a href="/blog/tags/astro4j.html">astro4j</a> 
	
		<a href="/blog/tags/solex.html">solex</a> 
	
		<a href="/blog/tags/java.html">java</a> 
	
		<a href="/blog/tags/graalvm.html">graalvm</a> 
	</p>
			
  			<p><div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>This blog introduces <a href="https://github.com/melix/astro4j">astro4j</a>, my latest toy project, a open source collection of libraries and applications for astronomy, written in Java.
In particular, I will discuss <a href="https://github.com/melix/astro4j/tree/main/jsolex">JSol&#8217;Ex</a>, a program aimed at reconstructing solar disk images from video files captured using the amazing <a href="http://www.astrosurf.com/solex/sol-ex-presentation-en.html">Sol&#8217;Ex</a> instrument.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_astro4j">Why astro4j?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>I&#8217;m a software developer, and if you are following me, you may also know that I&#8217;m an <a href="https://www.astrobin.com/users/melix/">amateur astrophotographer</a>.
For a long time, I&#8217;ve been fascinated by the quality of software we have in astronomy, to process images.
If you are french speaking, you can watch a <a href="https://www.youtube.com/watch?v=tSgnOtdjVHs">presentation I gave about this topic</a>.
Naturally, I have been curious about how all these things work, but it&#8217;s actually extremely rare to find open source software, and when you do, it&#8217;s rarely written in Java.
For example, both <a href="http://www.firecapture.de/">Firecapture</a> (software to capture video streams) and <a href="https://www.astropixelprocessor.com/">Astro Pixel Processor</a> are written in Java, but both of them are closed source, commercial software.</p>
</div>
<div class="paragraph">
<p>Last month, for my birthday, I got a <a href="http://www.astrosurf.com/solex/sol-ex-presentation-en.html">Sol&#8217;Ex</a>, an instrument which combines spectrography and software to realize amazing solar pictures in different spectral lines.
To process those images, the easiest solution is to use <a href="http://valerie.desnoux.free.fr/inti/">the amazing INTI software</a>, written in Python, but for which sources are not published, as far as I know, neither on GitHub or GitLab.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
After announcing this project, I have been notified that the sources of INTI <a href="https://github.com/Vdesnoux/Solex_ser_recon">are indeed available, as GPL</a>. It&#8217;s a pity they are not linked on the webpage, this would have helped a lot.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To give you an example of what you can do, here&#8217;s the <a href="https://www.astrobin.com/94gymd/">first photography</a> I&#8217;ve done with Sol&#8217;Ex and processed with INTI (color was added in Gimp):</p>
</div>
<a href="https://astrob.in/94gymd/0/"><img src="https://astrob.in/94gymd/0/rawthumb/regular/get.jpg?insecure"/></a>
<div class="paragraph">
<p>To get this result, one has to combine images which look like this:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/astro/solex/spectrum.png" alt="spectrum">
</div>
</div>
<div class="paragraph">
<p>Interesting, no?
At the same time, I was a bit frustrated by INTI.
While it clearly does the job and is extremely easy to use, there are a few things which I didn&#8217;t like:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>the first, which I mentioned, is that it&#8217;s using Python and that the sources are not published (as far as I understand, some algorithms are not published yet). I am not surprised that Python is used, because it&#8217;s a language which is extremely popular in academics, with lots of libraries for image processing, science oriented libs, etc. However, because it&#8217;s popular in academics also means that programs are often written by and for academics. When we&#8217;re talking about maths, it&#8217;s often short variable names, cryptic function names, etc&#8230;&#8203;</p>
</li>
<li>
<p>second, after processing, INTI pops up a lot of images as individual windows. If you want to process a new file, you have to close all of them. The problem is that I still haven&#8217;t figured out in which order you have to do this so that you can restart from the initial window which lets you select a video file! Apparently, depending on the order, it will, or will not, show the selector. And sometimes, it takes several seconds before it does so.</p>
</li>
<li>
<p>INTI seems to be regenerating a font cache every time I reboot. This operation takes several minutes. It&#8217;s probably an artifact of packaging the application for Windows, but still, not very user friendly.</p>
</li>
<li>
<p>INTI generates a number of images, but puts them alongside the videos. I like things organized (well, at least virtually, because if you looked at my desk right now, it is likely you&#8217;d feel faint), so I wish it was creating one directory per processed video.</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/astro/solex/inti-popups.png" alt="inti popups">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_confronting_the_old_demons">Confronting the old demons</h2>
<div class="sectionbody">
<div class="paragraph">
<p>When I started studying at University, back in 1998, I was planning to do astrophysics.
However, I quickly forgot about this idea when I saw the amount of maths one has to master to do modern physics.
Clearly, I was reaching my limits, and it was extremely complicated for me.
Fortunately, I had been doing software development for years already, because I started very young, on my father&#8217;s computer.
So I decided to switch to computer science, where I was reasonably successful.</p>
</div>
<div class="paragraph">
<p>However, not being able to do what I wanted to do has always been a frustration. It is still, today, to the point that a lot of what I&#8217;m reading is about this topic, but still, I lack the maths.</p>
</div>
<div class="paragraph">
<p>It was time for me to confront my old demons, and answer a few questions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>am I still capable of understanding maths, in order to implement algorithms which I use everyday when I do astronomy image processing with software written by others?</p>
</li>
<li>
<p>can I read academic papers, for example to implement a FFT (Fast Fourier Transform) algorithm, although I clearly remember that I failed to understand the principles when I was at school?</p>
</li>
<li>
<p>can I do this while writing something which could be useful to others, and publish it as open source software?</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Astro4j is there to answer those questions.
I don&#8217;t have the answers yet and time will tell if I&#8217;m successful.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_modern_java">Using modern Java</h2>
<div class="sectionbody">
<div class="paragraph">
<p>One question you may have is why Java? If you are not familiar with this language, you may have this old misconception that Java is slow.
It&#8217;s not. Especially, if you compare to Python, it&#8217;s definitely not.</p>
</div>
<div class="paragraph">
<p>This project is also for me a way to prove that you can implement "serious science" in Java.
You can already find some science libraries in Java, but they tend to me impractical to use, because not following the industry standards (e.g published on Maven Central) or platform-dependent.</p>
</div>
<div class="paragraph">
<p>I also wanted to leverage this to <em>learn something new</em>.
So this project:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>uses Java 17 (at least for libraries, so that they can be consumed by a larger number of developers, for applications I&#8217;m considering moving to Java 20)</p>
</li>
<li>
<p>uses <a href="https://openjfx.io/">JavaFX</a> (OpenJFX) for the application UI</p>
</li>
<li>
<p>experiments with the <a href="https://openjdk.org/jeps/438">Vector API</a> for faster processing</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>As I said, my initial goal is to obtain a software which can basically do what INTI does.
It is not a goal to make it faster, but if I can do it, I will.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_introducing_jsolex">Introducing JSol&#8217;Ex</h2>
<div class="sectionbody">
<div class="paragraph">
<p>After a few evenings (and a couple week-ends ;)), I already have something which performs <em>basic processing</em>, that is to say that it can process a SER video file and generate a reconstructed solar disk.
It does <strong>not</strong> perform geometry correction, nor tilt correction, like INTI does. It doesn&#8217;t generate shifted images either (for example the doppler images), but <strong>it works</strong>.</p>
</div>
<div class="paragraph">
<p>Since the only source of information I had to do this was <a href="http://www.astrosurf.com/solex/sol-ex-presentation-en.html">Christian Buil&#8217;s website</a> and <a href="http://valerie.desnoux.free.fr/inti/">Valérie Desnoux INTI&#8217;s website</a>, I basically had to implement my own algorithms from A to Z, and just "guess" how it works.</p>
</div>
<div class="paragraph">
<p>In order to do this, I had to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>implement a SER video file decoder. The library is <a href="https://github.com/melix/astro4j/tree/main/jserfile">ready</a> and performs both decoding the SER file and performs demosaicing of images</p>
</li>
<li>
<p>on top of the decoder, I implemented a <a href="https://github.com/melix/astro4j/tree/main/ser-player">SER file player</a>, which is still very basic at this stage, and uses JavaFX. This player can even be compiled to a native binary using <a href="https://www.graalvm.org/">GraalVM</a>!</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Here&#8217;s an example:</p>
</div>
 <video width="800" height="480" controls>
  <source src="https://melix.github.io/blog/img/astro/solex/serplayer.webm" type="video/webm">
Your browser does not support the video tag.
</video>
<div class="paragraph">
<p>Then I could finally start working on the Sol&#8217;Ex video processor.
As I said, I don&#8217;t know how INTI works, so this is all trial and error, in the end&#8230;&#8203;</p>
</div>
<div class="paragraph">
<p>In the beginning, as I said, you have a SER video file which contains a lot of frames (for example, in my case, it&#8217;s a file from 500MB to 1GB) that we have to process in order to generate a solar disk.
Each frame consists of a view of the light spectrum, centered on a particular spectral line.</p>
</div>
<div class="paragraph">
<p>For example, in the following image, we have the H-alpha spectral line:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/astro/solex/spectrum.png" alt="spectrum">
</div>
</div>
<div class="paragraph">
<p>Because of optics, you can see that the line is not horizontal: each frame is distorted.
Therefore, in order to reconstruct an image, we have to deal with that distortion first.
For this, we have to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>detect the spectral line in the frame, which I&#8217;m doing by implementing a simple contrast detection</p>
</li>
<li>
<p>perform a linear regression in order to compute a 2d order polynomial which models the distortion</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Note that before doing this, I had no idea how to do a 2d order regression, but I searched and found that it was possible to do so using the least squares method, so I did so.
The result is that we can identify precisely the line with this technique:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/astro/solex/spectrum-line.png" alt="spectrum line">
</div>
</div>
<div class="paragraph">
<p>In the beginning, I tought I would have to perform distortion correction in order to reconstruct the image, because I was (wrongly) assuming that, because each frame represents <em>one</em> line in the reconstructed image, I had to compute the average of the colums of each frame to determine the color of a <em>single</em> pixel in the output. I was wrong (we&#8217;ll come to that later), but I did implement a distortion correction algorithm:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/astro/solex/spectrum-corrected.png" alt="spectrum corrected">
</div>
</div>
<div class="paragraph">
<p>When I computed the average, the resulting image was far from the quality and constrast of what I got with INTI.
What a failure!
So I thought that maybe I had to compute the average of the spectral line itself.
I tried this, and indeed, the resulting image was much better, but still not the quality of INTI.
The last thing I did, therefore, was to pick the middle of the spectral line itself, and then, magically, I got the same level of quality as with INTI (for the raw images, as I said I didn&#8217;t implement any geometry or tilt correction yet).</p>
</div>
<div class="paragraph">
<p>The reason I was assuming that I had to compute an average, is that it wasn&#8217;t clear to me that the <em>absorption ray</em> would actually contain enough data to reconstruct an image.
As it was an absorption ray, I assumed that the value would be 0, and therefore that nothing would come out of using the ray itself.
In fact, my physics were wrong, and you <em>must</em> use that.</p>
</div>
<div class="paragraph">
<p>A direct consequence is that there is actually no need to perform a distortion correction.
Instead, you can just use the 2d order polynomial that we&#8217;ve computed, and "follow the line", that&#8217;s it!</p>
</div>
<div class="paragraph">
<p>Now, we can generate an image, but it will be very dark.
The reason is obvious: by taking the middle of the spectral line, we&#8217;re basically using dark pixels, so the dynamics of the image are extremely low.
So, in order to have something which "looks nice", you actually have to perform brightness correction.</p>
</div>
<div class="paragraph">
<p>The first algorithm I have used is simply a linear correction: we&#8217;re computing the max and min value of the image, then rescaling that so that the max value is the maximum representable (255).</p>
</div>
<div class="paragraph">
<p>Here&#8217;s the result:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/astro/solex/linear.png" alt="linear">
</div>
</div>
<div class="paragraph">
<p>However, I felt that this technique wouldn&#8217;t give the best results, in particular because linear images tend to give results which are not what the eye would see: our eye performs a bit like an "exponential" accumulator, the more photos you get, the "brighter" we&#8217;ll see it.</p>
</div>
<div class="paragraph">
<p>So I implemented another algorithm which I had seen in <a href="https://pixinsight.com/">PixInsight</a>, which is called inverse hyperbolic (Arcsinh) correction:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/astro/solex/streched.png" alt="streched">
</div>
</div>
<div class="paragraph">
<p>Last, you can see that the image has lots of vertical line artifacts.
This is due to the presence of dust either on the optics or the sensors.
INTI performs correction of those lines, and I wanted to do something similar.</p>
</div>
<div class="paragraph">
<p>Again, I don&#8217;t know what INTI is doing, so I figured out my own technique, which is using "multipass" correction.
In a nutshell, for each row, I am computing the average value of the row.
Then, for a particular row, I compute the average of the averages of the surrounding lines (for example, 16 rows before and after).
If the average of this line is <em>below</em> the average of the averages(!), then I&#8217;m considering that the line is darker than it should be, computing a correction factor and applying it.</p>
</div>
<div class="paragraph">
<p>The result is a corrected image:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/blog/img/astro/solex/banding.png" alt="banding">
</div>
</div>
<div class="paragraph">
<p>We&#8217;re still not a the level of quality that INTI produces, but getting close!</p>
</div>
<div class="paragraph">
<p>So what&#8217;s next? I already have <a href="https://github.com/melix/astro4j/issues">added some issues for things I want to fix</a>, and in particular, I&#8217;m looking at improving the banding reduction and performing geometry correction.
For both, I <em>think</em> I will need to use fast fourier transforms, in order to identify the noise in one case (banding) and detect edges in the other (geometry correction).</p>
</div>
<div class="paragraph">
<p>Therefore, I started to implement FFT transforms, a domain I had absolutely no knowledge of.
Luckily, I could ask <a href="https://chat.openai.com/">ChatGPT</a> to explain to me the concepts, which made it faster to implement!
For now, I have only implemented the <a href="https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm">Cooley-Tukey</a> algorithm.
The issue is that this algorithm is quite slow, and requires that the input data has a length which is a power of 2.
Given the size of the image we generate, it&#8217;s quite costly.</p>
</div>
<div class="paragraph">
<p>I took advantage of this to learn about the <a href="https://openjdk.org/jeps/438">Vector API</a> to leverage SIMD instructions of modern CPUs, and it indeed made things significantly faster (about twice as fast), but still not at the level of performance that I expect.</p>
</div>
<div class="paragraph">
<p>I am trying to understand the <a href="https://en.wikipedia.org/wiki/Split-radix_FFT_algorithm">split radix</a> but I&#8217;m clearly intimidated by the many equations here&#8230;&#8203; In any case I printed some papers which I hope I&#8217;ll be able to understand.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_conclusion">Conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In conclusion, in this article, I&#8217;ve introduced <a href="https://github.com/melix/astro4j">astro4j</a>, an open source suite of libraries and applications written in Java for astronomy software.
While the primary goal for me is to learn and improve my skills and knowledge of the maths behind astronomy software processing, it <em>may</em> be that it produces something useful.
In any case, since it&#8217;s open source, if you want to contribute, feel free!</p>
</div>
<div class="paragraph">
<p>And you can do so in different domains, for example, I pretty much s* at UI, so if you are a JavaFX expert, I would appreciate your pull requests!</p>
</div>
<div class="paragraph">
<p>Finally, here is a video showing JSol&#8217;Ex in action:</p>
</div>
 <video width="1024" height="768" controls>
  <source src="https://melix.github.io/blog/img/astro/solex/jsolex.webm" type="video/webm">
Your browser does not support the video tag.
</video>
</div>
</div></p>
  			<a href="/blog/2023/03-12-micronaut-catalogs.html"><h1>How the Micronaut team leverages Gradle&#8217;s version catalogs for improved developer productivity</h1></a>
  			<p>12 March 2023</p>
			<p><em>Tags: </em>
		<a href="/blog/tags/micronaut.html">micronaut</a> 
	
		<a href="/blog/tags/gradle.html">gradle</a> 
	
		<a href="/blog/tags/version-catalogs.html">version catalogs</a> 
	
		<a href="/blog/tags/graalvm.html">graalvm</a> 
	
		<a href="/blog/tags/maven.html">maven</a> 
	</p>
			
  			<p><div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>This blog post discusses how the Micronaut development team makes use of a feature of Gradle, <a href="https://docs.gradle.org/current/userguide/platforms.html">version catalogs</a>, to improve the team&#8217;s developer productivity, reduce the risks of publishing broken releases, coordinate the releases of a large number of modules and, last but not least, provide additional features to our Gradle users.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_backstory">The backstory</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <a href="https://micronaut.io/">Micronaut Framework</a> is a modern open-source framework for building JVM applications.
It can be used to build all kinds of applications, from CLI applications to microservices or even good old monoliths.
It supports deploying both to the JVM and native executables (using <a href="https://graalvm.org">GraalVM</a>), making it particularly suitable for all kind of environments.
A key feature of the Micronaut framework is developer productivity: we do everything we can to make things faster for developers.
In particular, Micronaut has a strong emphasis on easing how you test your applications, even in native mode.
For this we have built a number of tools, including our <a href="https://micronaut-projects.github.io/micronaut-maven-plugin/latest/">Maven</a> and <a href="https://micronaut-projects.github.io/micronaut-gradle-plugin/latest/">Gradle</a> plugins.</p>
</div>
<div class="paragraph">
<p>When I joined the Micronaut team almost a couple years back, I was given the responsibility of improving the team&#8217;s own developer productivity.
It was an exciting assignment, not only because I knew the team&#8217;s love about Gradle, but because I also knew that there were many things we could do to reduce the feedback time, to provide more insights about failures, to detect flaky tests, etc.
As part of this work we have put in place a partnership with <a href="https://gradle.com">Gradle Inc</a> which kindly provides us with a <a href="https://ge.micronaut.io">Gradle Enterprise instance</a>, but this is not what I want to talk about today.</p>
</div>
<div class="paragraph">
<p>Lately I was listening to an <a href="https://www.youtube.com/watch?v=Gr96IxKwPeE">interview of Aurimas Liutikas</a> of the AndroidX team, who was saying that he didn&#8217;t think that version catalogs were a good solution for library authors to share their recommendations of versions, and that BOMs are probably a better solution for this.
I pinged him saying that I disagreed with this statement and offered to provide more details why, if he was interested.
This is therefore a long answer, but one which will be easier to find than a <a href="https://androiddev.social/@Aurimas/110000457198553518">thread on social media</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_what_are_version_catalogs">What are version catalogs?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Let&#8217;s start with the basics: a version catalog is, like the name implies, a catalog of versions to pick from, nothing more.
That doesn&#8217;t sound too much exciting, and what versions are we talking about?
That&#8217;s version of <em>libraries</em> or <em>plugins</em> that you use in your build.</p>
</div>
<div class="paragraph">
<p>As an illustration, here is a version catalog, defined as a TOML file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="toml">[versions]
javapoet = "1.13.0"

[libraries]
javapoet = { module = "com.squareup:javapoet", version.ref = "javapoet" }</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then this library <em>can</em> be used in a <code>dependencies</code> declaration block in any of the project&#8217;s build script using a <em>type-safe notation</em>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="gradle">dependencies {
    implementation(libs.javapoet) {
        because("required for Java source code generation")
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>which is <em>strictly equivalent</em> to writing:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="gradle">dependencies {
    implementation("com.squareup:javapoet:1.13.0") {
        because("required for Java source code generation")
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>There are many advantages of using version catalogs to declare your library versions, but most notably it provides a single, standard location where those versions are declared.
It is important to understand that a catalog is simply a <em>list of dependencies you can pick from</em>, a bit like going to the supermarket and choosing whatever you need for your particular meal: it&#8217;s not because a catalog declares libraries that you <em>have to</em> use them.
However, a catalog provides you with <em>recommendations</em> of libraries to pick from.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_version_catalogs_for_micronaut_users">Version catalogs for Micronaut users</h2>
<div class="sectionbody">
<div class="paragraph">
<p>An interesting aspect of version catalogs is that they can be published, for others to consume: they are an artifact.
Micronaut users can already make use of catalogs, as I have explained in a <a href="https://melix.github.io/blog/2022/02/micronaut-version-catalog.html">previous blog post</a>.
This makes it possible for a user who doesn&#8217;t know which version of Micronaut Data to use, to simply declare:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="gradle">dependencies {
    implementation mn.micronaut.data
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>People familiar with Maven BOMs can easily think that it is the same feature, but there are <a href="https://docs.gradle.org/current/userguide/platforms.html#sub:platforms-vs-catalog">key differences which are described in the Gradle docs</a>.</p>
</div>
<div class="paragraph">
<p>In the rest of this post we will now focus on how we generate those catalogs, and how they effectively help us in improving our own developer productivity.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_how_the_micronaut_team_uses_version_catalogs">How the Micronaut team uses version catalogs</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_one_catalog_per_module">One catalog per module</h3>
<div class="paragraph">
<p>As I said, the Micronaut framework consists of a large number of modules which live in their own Git repository.
All the projects share the same layout, the same conventions in order to make things easier to maintain.
For this purpose, we use our own collection of <a href="https://github.com/micronaut-projects/micronaut-build">internal build plugins</a> as well as a <a href="https://github.com/micronaut-projects/micronaut-project-template">project template</a>.</p>
</div>
<div class="paragraph">
<p>Those build plugins provide features like:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>defining the default Java language level, setting up code conventions and code quality plugins</p>
</li>
<li>
<p>standardizing how documentation is built (using Asciidoctor)</p>
</li>
<li>
<p>setting up integration with Gradle Enterprise, to publish build scans, configure the build cache and predictive test selection</p>
</li>
<li>
<p>implementing binary compatibility checks between releases</p>
</li>
<li>
<p>configuring publication to Maven Central</p>
</li>
<li>
<p>providing a high-level model of what a Micronaut module is</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The last item is particularly important: in every Micronaut project, we have different kind of modules: libraries (which are published to Maven Central for users to consume), internal support libraries (which are not intended for external consumption), or a BOM module (which also publishes a version catalog as we&#8217;re going to see).</p>
</div>
<div class="paragraph">
<p>Long story short: <strong>we heavily rely on conventions to reduce the maintenance costs, have consistent builds, with improved performance and higher quality standards</strong>.
If you are interested in why we have such plugins, Sergio Delamo and I gave an <a href="https://www.youtube.com/watch?v=fpz63IwFIZM">interview about this</a> a few months ago (alert: the thumbnail shows I have hair, this is fake news!).</p>
</div>
<div class="paragraph">
<p>Each of our projects declares a version catalog, for example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>this <a href="https://github.com/micronaut-projects/micronaut-core/blob/4.0.x/gradle/libs.versions.toml">one for Micronaut core</a></p>
</li>
<li>
<p>this <a href="https://github.com/micronaut-projects/micronaut-test-resources/blob/master/gradle/libs.versions.toml">one for Micronaut Test Resources</a></p>
</li>
<li>
<p>this <a href="https://github.com/micronaut-projects/micronaut-kafka/blob/master/gradle/libs.versions.toml">one for Micronaut Kafka</a></p>
</li>
<li>
<p>or this <a href="https://github.com/micronaut-projects/micronaut-gradle-plugin/blob/master/gradle/libs.versions.toml">one for the Micronaut Gradle Plugin</a></p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_automatic_version_upgrades">Automatic version upgrades</h3>
<div class="paragraph">
<p>One of the advantages of version catalogs is that it provides a centralized place for versions, which can be easily used by bots to provide pull requests for dependency upgrades.
For this, we use <a href="https://docs.renovatebot.com">Renovatebot</a> which integrates particularly well with version catalogs (GitHub&#8217;s dependabot lacks behind in terms of support).
This allows us to get pull requests like <a href="https://github.com/micronaut-projects/micronaut-kafka/pull/660/files">this one</a> which are very easy to review.</p>
</div>
</div>
<div class="sect2">
<h3 id="_bom_and_version_catalog_generation">BOM and version catalog generation</h3>
<div class="paragraph">
<p>Each of the Micronaut projects is now required to provide a BOM (Bill of Materials) for users.
Another term for a BOM that is used in the Gradle ecosystem is a <em>platform</em>: a platform has however slightly different semantics in Maven and Gradle.
The main goal of a BOM is to provide a list of dependencies a project works with, and, in Maven, it <em>can</em> be used to override the dependency versions of transitive dependencies.
While in Maven, a BOM will only influence the dependency resolution of the project which <em>imports</em> the BOM, in Gradle a platform fully participates in dependency resolution, including when a transitive dependency depends on a a BOM.
To simplify, a user who <em>imports</em> a BOM may use dependencies declared in the BOM <em>without specifying a version</em>: the version will be fetched from the BOM.
In that regards, it looks exactly the same as a version catalog, but there are subtle differences.</p>
</div>
<div class="paragraph">
<p>For example, if a user imports a BOM, any transitive dependency matching a dependency found in the BOM will be overridden (Maven) or participate in conflict resolution (Gradle).
That is <em>not</em> the case for a catalog: it will <em>not</em> influence the dependency resolution unless you explicitly add a dependency which belongs to the catalog.</p>
</div>
<div class="paragraph">
<p>That&#8217;s why Micronaut publishes <em>both</em> a BOM and a catalog, because they address different use cases, and they work particularly well when combined together.</p>
</div>
<div class="paragraph">
<p>In Micronaut modules, you will systematically find a project with the <code>-bom</code> suffix.
For example, Micronaut Security will have subprojects like <a href="https://github.com/micronaut-projects/micronaut-security/tree/master/security-jwt"><code>micronaut-security-jwt</code></a>, <a href="https://github.com/micronaut-projects/micronaut-security/tree/master/security-oauth2"><code>micronaut-security-oauth2</code></a> and <a href="https://github.com/micronaut-projects/micronaut-security/tree/master/security-bom"><code>micronaut-security-bom</code></a>.</p>
</div>
<div class="paragraph">
<p>The BOM project will aggregate dependencies used by the different modules.
In order to publish a BOM file, the only thing a project has to do is to apply our convention plugin:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="gradle">plugins {
    id "io.micronaut.build.internal.bom"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note how we don&#8217;t have to declare the coordinates of the BOM (group, artifact, version), nor that we have to declare how to publish to Maven Central, what dependencies should be included in the BOM, etc: <em>everything</em> is done by convention, that&#8217;s the magic of <a href="https://melix.github.io/blog/2021/12/composition-in-gradle.html">composition over inheritance</a>.</p>
</div>
<div class="paragraph">
<p>Should we want to change how we generate the BOM, the only thing we would have to do is to update our internal convention plugin, then all projects would benefit from the change once they upgrade.</p>
</div>
</div>
<div class="sect2">
<h3 id="_convention_over_configuration">Convention over configuration</h3>
<div class="paragraph">
<p>In order to determine which dependencies should be included in our BOM, we defined <em>conventions</em> that we use in our catalog files.
In our internal terminology, when we want a dependency to be handled by the Micronaut framework, we call that a <em>managed</em> dependency: a dependency that is managed by Micronaut and that users shouldn&#8217;t care about in most cases: they don&#8217;t have to think about a version, we will provide one for them.</p>
</div>
<div class="paragraph">
<p>This directly translates to a convention in the version catalogs of the Micronaut projects: dependencies which are <em>managed</em> need to be declared with a <code>managed-</code> prefix in the catalog:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="toml">[versions]
...
managed-kafka = '3.4.0'
...
zipkin-brave-kafka-clients = '5.15.0'

[libraries]
...
managed-kafka-clients = { module = 'org.apache.kafka:kafka-clients', version.ref = 'managed-kafka' }
managed-kafka-streams = { module = 'org.apache.kafka:kafka-streams', version.ref = 'managed-kafka' }
...
zipkin-brave-kafka-clients = { module = 'io.zipkin.brave:brave-instrumentation-kafka-clients', version.ref = 'zipkin-brave-kafka-clients' }</code></pre>
</div>
</div>
<div class="paragraph">
<p>Those dependencies will end up in the version catalog that we generate, but <em>without</em> the <code>managed-</code> prefix.
This means that we would generate a BOM which contains the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&gt;
  &lt;!-- This module was also published with a richer model, Gradle metadata,  --&gt;
  &lt;!-- which should be used instead. Do not delete the following line which  --&gt;
  &lt;!-- is to indicate to Gradle or any Gradle module metadata file consumer  --&gt;
  &lt;!-- that they should prefer consuming it instead. --&gt;
  &lt;!-- do_not_remove: published-with-gradle-metadata --&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  &lt;groupId&gt;io.micronaut.kafka&lt;/groupId&gt;
  &lt;artifactId&gt;micronaut-kafka-bom&lt;/artifactId&gt;
  &lt;version&gt;5.0.0-SNAPSHOT&lt;/version&gt;
  &lt;packaging&gt;pom&lt;/packaging&gt;
  &lt;name&gt;Micronaut Kafka&lt;/name&gt;
  &lt;description&gt;Integration between Micronaut and Kafka Messaging&lt;/description&gt;
  &lt;url&gt;https://micronaut.io&lt;/url&gt;
  &lt;licenses&gt;
    &lt;license&gt;
      &lt;name&gt;The Apache Software License, Version 2.0&lt;/name&gt;
      &lt;url&gt;http://www.apache.org/licenses/LICENSE-2.0.txt&lt;/url&gt;
      &lt;distribution&gt;repo&lt;/distribution&gt;
    &lt;/license&gt;
  &lt;/licenses&gt;
  &lt;scm&gt;
    &lt;url&gt;scm:git@github.com:micronaut-projects/micronaut-kafka.git&lt;/url&gt;
    &lt;connection&gt;scm:git@github.com:micronaut-projects/micronaut-kafka.git&lt;/connection&gt;
    &lt;developerConnection&gt;scm:git@github.com:micronaut-projects/micronaut-kafka.git&lt;/developerConnection&gt;
  &lt;/scm&gt;
  &lt;developers&gt;
    &lt;developer&gt;
      &lt;id&gt;graemerocher&lt;/id&gt;
      &lt;name&gt;Graeme Rocher&lt;/name&gt;
    &lt;/developer&gt;
  &lt;/developers&gt;
  &lt;properties&gt;
    &lt;micronaut.kafka.version&gt;5.0.0-SNAPSHOT&lt;/micronaut.kafka.version&gt;
    &lt;kafka.version&gt;3.4.0&lt;/kafka.version&gt;
  &lt;/properties&gt;
  &lt;dependencyManagement&gt;
    &lt;dependencies&gt;
      &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
        &lt;version&gt;${kafka.compat.version}&lt;/version&gt;
      &lt;/dependency&gt;
      &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;
        &lt;version&gt;${kafka.version}&lt;/version&gt;
      &lt;/dependency&gt;
      &lt;dependency&gt;
        &lt;groupId&gt;io.micronaut.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;micronaut-kafka&lt;/artifactId&gt;
        &lt;version&gt;${micronaut.kafka.version}&lt;/version&gt;
      &lt;/dependency&gt;
      &lt;dependency&gt;
        &lt;groupId&gt;io.micronaut.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;micronaut-kafka-streams&lt;/artifactId&gt;
        &lt;version&gt;${micronaut.kafka.version}&lt;/version&gt;
      &lt;/dependency&gt;
    &lt;/dependencies&gt;
  &lt;/dependencyManagement&gt;
&lt;/project&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note how we automatically translated the <code>managed-kafka</code> property into a BOM property <code>kafka.version</code>, which is used in the <code>&lt;dependencyManagement&gt;</code> block.
Dependencies which do <em>not</em> start with <code>managed-</code> <strong>are not included</strong> in our generated BOM.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s now look at the version catalog that we generate:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="toml">#
# This file has been generated by Gradle and is intended to be consumed by Gradle
#
[metadata]
format.version = "1.1"

[versions]
kafka = "3.4.0"
kafka-compat = "3.4.0"
micronaut-kafka = "5.0.0-SNAPSHOT"

[libraries]
kafka = {group = "org.apache.kafka", name = "kafka-clients", version.ref = "kafka-compat" }
kafka-clients = {group = "org.apache.kafka", name = "kafka-clients", version.ref = "kafka" }
kafka-streams = {group = "org.apache.kafka", name = "kafka-streams", version.ref = "kafka" }
micronaut-kafka = {group = "io.micronaut.kafka", name = "micronaut-kafka", version.ref = "micronaut-kafka" }
micronaut-kafka-bom = {group = "io.micronaut.kafka", name = "micronaut-kafka-bom", version.ref = "micronaut-kafka" }
micronaut-kafka-streams = {group = "io.micronaut.kafka", name = "micronaut-kafka-streams", version.ref = "micronaut-kafka" }</code></pre>
</div>
</div>
<div class="paragraph">
<p>Given a <em>single</em> input, the version catalog that we use to build our Micronaut module, our build conventions let us automatically declare which dependencies should land in the <em>output</em> BOM and version catalogs that we generate for that project!
Unlike Maven BOMs which either <em>have to</em> be a parent POM <em>or</em> redeclare all dependencies in an independent module, in Gradle we can generate these automatically and completely decouple the output BOM from what is required to build our project.</p>
</div>
<div class="paragraph">
<p>In general, all <em>api</em> dependencies must be managed, so in the example above, the Micronaut Kafka build scripts would have an API dependency on <code>kafka-clients</code>, which we can find in the main project build script:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="gradle">dependencies {
    api libs.managed.kafka.clients
    ...
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The benefit of generating a version catalog for a user is that there is now a <a href="https://repo1.maven.org/maven2/io/micronaut/kafka/micronaut-kafka-bom/4.5.2/micronaut-kafka-bom-4.5.2.toml">Micronaut Kafka version catalog published on Maven Central</a>, alongside the BOM file.</p>
</div>
<div class="paragraph">
<p>This catalog can be imported by a user in their settings file:</p>
</div>
<div class="listingblock">
<div class="title">settings.gradle</div>
<div class="content">
<pre class="pygments highlight"><code data-lang="gradle">dependencyResolutionManagement {
    versionCatalogs {
         create("mnKafka") {
             from("io.micronaut.kafka:micronaut-kafka-bom:4.5.2")
         }
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then the dependency on Micronaut Kafka and its managed dependencies can be used in a build script using the <code>mnKafka</code> prefix:</p>
</div>
<div class="listingblock">
<div class="title">build.gradle</div>
<div class="content">
<pre class="pygments highlight"><code data-lang="gradle">dependencies {
    implementation mnKafka.micronaut.kafka
    implementation mnKafka.kafka.clients
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>A user doesn&#8217;t have to know about the dependency coordinates of Kafka clients: the IDE (at least IntelliJ IDEA) would provide completion automatically!</p>
</div>
</div>
<div class="sect2">
<h3 id="_bom_composition">BOM composition</h3>
<div class="paragraph">
<p>In Micronaut 3.x, there is a problem that we intend to fix in Micronaut 4 regarding our "main" BOM: the Micronaut core BOM is considered as our "platform" BOM, in the sense that it aggregates BOMs of various Micronaut modules.
This makes it hard to release newer versions of Micronaut which, for example, only upgrade particular modules of Micronaut.</p>
</div>
<div class="paragraph">
<p>Therefore in Micronaut 4, we are cleanly separating the "core" BOM, from the new <a href="https://github.com/micronaut-projects/micronaut-platform">platform BOM</a>.
It is interesting in this blog post because it offers us the opportunity to show how we are capable of generating <em>aggregating BOMs</em> and <em>aggregated catalogs</em>.</p>
</div>
<div class="paragraph">
<p>In the platform BOM module, you can find the <a href="https://github.com/micronaut-projects/micronaut-platform/blob/master/gradle/libs.versions.toml">"input" catalog</a> that we use, and only consists of <code>managed-</code> dependencies.
Most of those dependencies are simply dependencies on other Micronaut BOMs: this is an "aggregating" BOM, which imports other BOMs.
This is, therefore, the only BOM that a user would effectively have to use when migrating to Micronaut 4: instead of importing all BOMs for the different Micronaut modules they use, they can simply import the Micronaut Platform BOM, which will then automatically include the BOMs of other modules which "work well together".</p>
</div>
<div class="paragraph">
<p>This allows us to <strong>decouple the releases</strong> of the framework from the releases of Micronaut core itself.</p>
</div>
<div class="paragraph">
<p>However, there is a subtlety about aggregating BOMs in Maven: they are not regular dependencies, but dependencies with the <code>import</code> scope.
This means that we must make a difference between a "managed dependency" and an "imported BOM" in our input catalog.</p>
</div>
<div class="paragraph">
<p>To do this, we have <em>another</em> naming convention, which is to use the <code>boms-</code> prefix for imported BOMs:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="toml">[versions]
...
managed-micronaut-aws = "4.0.0-SNAPSHOT"
managed-micronaut-azure = "5.0.0-SNAPSHOT"
managed-micronaut-cache = "4.0.0-SNAPSHOT"
managed-micronaut-core = "4.0.0-SNAPSHOT"
...

[libraries]
...
boms-micronaut-aws = { module = "io.micronaut.aws:micronaut-aws-bom", version.ref = "managed-micronaut-aws" }
boms-micronaut-azure = { module = "io.micronaut.azure:micronaut-azure-bom", version.ref = "managed-micronaut-azure" }
boms-micronaut-cache = { module = "io.micronaut.cache:micronaut-cache-bom", version.ref = "managed-micronaut-cache" }
boms-micronaut-core = { module = "io.micronaut:micronaut-core-bom", version.ref = "managed-micronaut-core" }
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>This results in the following BOM file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  &lt;groupId&gt;io.micronaut.platform&lt;/groupId&gt;
  &lt;artifactId&gt;micronaut-platform&lt;/artifactId&gt;
  &lt;version&gt;4.0.0-SNAPSHOT&lt;/version&gt;
  &lt;packaging&gt;pom&lt;/packaging&gt;
  &lt;name&gt;Micronaut Platform&lt;/name&gt;
  &lt;description&gt;Bill-Of-Materials (BOM) and Gradle version catalogs for Micronaut&lt;/description&gt;

  ...

  &lt;properties&gt;
    ...
    &lt;micronaut.aws.version&gt;4.0.0-SNAPSHOT&lt;/micronaut.aws.version&gt;
    &lt;micronaut.azure.version&gt;5.0.0-SNAPSHOT&lt;/micronaut.azure.version&gt;
    &lt;micronaut.cache.version&gt;4.0.0-SNAPSHOT&lt;/micronaut.cache.version&gt;
    &lt;micronaut.core.version&gt;4.0.0-SNAPSHOT&lt;/micronaut.core.version&gt;
    ...
  &lt;/properties&gt;
  &lt;dependencyManagement&gt;
    &lt;dependencies&gt;
      ...
      &lt;dependency&gt;
        &lt;groupId&gt;io.micronaut.aws&lt;/groupId&gt;
        &lt;artifactId&gt;micronaut-aws-bom&lt;/artifactId&gt;
        &lt;version&gt;${micronaut.aws.version}&lt;/version&gt;
        &lt;type&gt;pom&lt;/type&gt;
        &lt;scope&gt;import&lt;/scope&gt;
      &lt;/dependency&gt;
      &lt;dependency&gt;
        &lt;groupId&gt;io.micronaut.azure&lt;/groupId&gt;
        &lt;artifactId&gt;micronaut-azure-bom&lt;/artifactId&gt;
        &lt;version&gt;${micronaut.azure.version}&lt;/version&gt;
        &lt;type&gt;pom&lt;/type&gt;
        &lt;scope&gt;import&lt;/scope&gt;
      &lt;/dependency&gt;
      &lt;dependency&gt;
        &lt;groupId&gt;io.micronaut.cache&lt;/groupId&gt;
        &lt;artifactId&gt;micronaut-cache-bom&lt;/artifactId&gt;
        &lt;version&gt;${micronaut.cache.version}&lt;/version&gt;
        &lt;type&gt;pom&lt;/type&gt;
        &lt;scope&gt;import&lt;/scope&gt;
      &lt;/dependency&gt;
      &lt;dependency&gt;
        &lt;groupId&gt;io.micronaut&lt;/groupId&gt;
        &lt;artifactId&gt;micronaut-core-bom&lt;/artifactId&gt;
        &lt;version&gt;${micronaut.core.version}&lt;/version&gt;
        &lt;type&gt;pom&lt;/type&gt;
        &lt;scope&gt;import&lt;/scope&gt;
      &lt;/dependency&gt;
      ...
    &lt;/dependencies&gt;
  &lt;/dependencyManagement&gt;
&lt;/project&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>A more interesting topic to discuss is what we can do with version catalogs that we publish for users: we can <strong>inline dependency aliases</strong> from each of the imported catalogs into the platform catalog.
All dependencies in the catalog files of each modules are directly available in the platform catalog:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="toml">[versions]
dekorate = "1.0.3"
elasticsearch = "7.17.8"
...
micronaut-aws = "4.0.0-SNAPSHOT"
micronaut-azure = "5.0.0-SNAPSHOT"
micronaut-cache = "4.0.0-SNAPSHOT"
micronaut-core = "4.0.0-SNAPSHOT"
...

[libraries]
alexa-ask-sdk = {group = "com.amazon.alexa", name = "ask-sdk", version = "" }
alexa-ask-sdk-core = {group = "com.amazon.alexa", name = "ask-sdk-core", version = "" }
alexa-ask-sdk-lambda = {group = "com.amazon.alexa", name = "ask-sdk-lambda-support", version = "" }
aws-java-sdk-core = {group = "com.amazonaws", name = "aws-java-sdk-core", version = "" }
aws-lambda-core = {group = "com.amazonaws", name = "aws-lambda-java-core", version = "" }
aws-lambda-events = {group = "com.amazonaws", name = "aws-lambda-java-events", version = "" }
aws-serverless-core = {group = "com.amazonaws.serverless", name = "aws-serverless-java-container-core", version = "" }
awssdk-secretsmanager = {group = "software.amazon.awssdk", name = "secretsmanager", version = "" }
azure-cosmos = {group = "com.azure", name = "azure-cosmos", version = "" }
azure-functions-java-library = {group = "com.microsoft.azure.functions", name = "azure-functions-java-library", version = "" }
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>alexa-ask-sdk</code> is for example an alias which was originally declared in the <code>micronaut-aws</code> module.
Because we aggregate all catalogs, we can inline those aliases and make them directly available in user build scripts:</p>
</div>
<div class="listingblock">
<div class="title">settings.gradle</div>
<div class="content">
<pre class="pygments highlight"><code data-lang="gradle">dependencyResolutionManagement {
    versionCatalogs {
         create("mnKafka") {
             from("io.micronaut.platform:micronaut-platform:4.0.0-SNAPSHOT")
         }
    }
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">build.gradle</div>
<div class="content">
<pre class="pygments highlight"><code data-lang="gradle">dependencies {
...
    implementation(mn.micronaut.aws.alexa)
    implementation(mn.alexa.sdk)
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Generating a version catalog offers us a very pragmatic way to define all dependencies that users can use in their build scripts with guarantees that they work well together.</p>
</div>
</div>
<div class="sect2">
<h3 id="_technical_details">Technical details</h3>
<div class="paragraph">
<p>If you survived reading up to this point, you may be interested in learning how, technically, we implemented this.
You can take a look at our <a href="https://github.com/micronaut-projects/micronaut-build">internal build plugins</a>, but more specifically at the <a href="https://github.com/micronaut-projects/micronaut-build/blob/master/src/main/groovy/io/micronaut/build/MicronautBomPlugin.java">BOM plugin</a>.</p>
</div>
<div class="paragraph">
<p>In order to generate our BOM and version catalogs, we have mainly 2 inputs:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>the list of subprojects which need to participate in the BOM: in a Micronaut modules, we explained that we have several kinds of projects: libraries which are published, test suites, etc. Only a subset of these need to belong to the BOM, and we can determine that list automatically because each project applies a <em>convention plugin</em> which determines its kind. Only projects of a particular kind are included. Should exceptions be required, we have a <code>MicronautBomExtension</code> which allows us to configure more precisely what to include or not, via a nice DSL.</p>
</li>
<li>
<p>the list of dependencies, which is determined from the project&#8217;s version catalog</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>One issue is that while Gradle provides automatically the generated, type-safe accessors for version catalogs, there is actually no built-in model that you can access to represent the catalog <em>model</em> itself (what is an alias, references to versions, etc): the type-safe API represents a "realized" catalog, but not a low-level model that we can easily manipulate.
This means that we had to implement our <a href="https://github.com/micronaut-projects/micronaut-build/blob/master/src/main/java/io/micronaut/build/catalogs/internal/VersionCatalogTomlModel.java#L29">own model for this</a>.</p>
</div>
<div class="paragraph">
<p>We have also seen that we can generate a single platform, aggregating all Micronaut modules for a release, that the users can import into their build scripts.
Unfortunately it is not the case for the Micronaut modules themselves: for example, Micronaut Core <em>must not</em> depend on other Micronaut modules, but, for example, Micronaut Data can depend on Micronaut SQL and use dependencies from the Micronaut SQL catalog.
Those modules <em>cannot</em> depend on the platform BOM, because this is the aggregating BOM, so we would create a <em>cyclic dependency</em> and wouldn&#8217;t be able to release any module.</p>
</div>
<div class="paragraph">
<p>To mitigate this problem, our internal build plugins expose a DSL which allows each projects to declare which other modules they use:</p>
</div>
<div class="listingblock">
<div class="title">settings.gradle</div>
<div class="content">
<pre class="pygments highlight"><code data-lang="gradle">micronautBuild {
    importMicronautCatalog() // exposes a `mn` catalog
    importMicronautCatalog("micronaut-reactor") // exposes a `mnReactor` catalog
    importMicronautCatalog("micronaut-rxjava2") // exposes a `mnRxjava2` catalog
    ...
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>While this is simple from the <em>declaration site</em> point of view, it is less practical from a <em>consuming</em> point of view, since it forces us to use <em>different namespaces</em> for each imported catalog:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="gradle">dependencies {
    ...
    testImplementation mn.micronaut.inject.groovy
    testImplementation mnRxjava2.micronaut.rxjava2
    ...
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>It would have been better if we could actually merge several catalogs into a single one, but unfortunately that feature <a href="https://github.com/gradle/gradle/issues/20383">has been removed from Gradle</a>.
I still have hope that this will eventually be implemented, because not having this creates unnecessary boilerplate in build scripts and redundancy in names (e.g <code>implementation mnValidation.micronaut.validation</code>).</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_additional_benefits_and_conclusion">Additional benefits and conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>All that I described in this article aren&#8217;t the only benefits that we have on standardizing on version catalogs.
For example, we have tasks which allow us to check that our generated BOM files only reference dependencies which are actually published on Maven Central, or that there are no SNAPSHOT dependencies when we perform a release.
In the end, while most of the Micronaut developers had no idea what a version catalog was when I joined the team, all of them pro-actively migrated projects to use them because, I think, they immediately saw the benefits and value.
It also streamlined the dependency upgrade process which was still a bit cumbersome before, despite using dependabot.</p>
</div>
<div class="paragraph">
<p>We now have a very pragmatic way to both use catalogs for building our own projects, and generating BOMs and version catalogs which can be used by both our Maven and Gradle users.
Of course, only the Gradle users will benefit from the version catalogs, but we did that in a way which doesn&#8217;t affect our Maven users (and if you use Maven, I strongly encourage you to evaluate building Micronaut projects with Gradle instead, since the UX is much better).</p>
</div>
<div class="paragraph">
<p>I cannot end this blog post without mentioning a "problem" that we have today, which is that if you use <a href="https://micronaut.io/launch">Micronaut Launch</a> to generate a Micronaut project, then it will <em>not</em> use version catalogs.
We have an <a href="https://github.com/micronaut-projects/micronaut-starter/issues/1385">issue for this</a> and pull requests are very welcome!</p>
</div>
</div>
</div></p>
  			<a href="/blog/2023/gradle-synthetic-projects.html"><h1>Gradle&#8217;s flexibility in action</h1></a>
  			<p>06 February 2023</p>
			<p><em>Tags: </em>
		<a href="/blog/tags/gradle.html">gradle</a> 
	
		<a href="/blog/tags/micronaut.html">micronaut</a> 
	</p>
			
  			<p><div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>I often say that flexibility isn&#8217;t the reason why you should select Gradle to build your projects: reliability, performance, reproducibility, testability are better reasons.
There are, however, cases were its flexibility comes in handy, like last week, when a colleague of mine asked me how we could benchmark a Micronaut project using a variety of combination of features and Java versions.
For example, he wanted to compare the performance of an application with and without epoll enabled, with and without Netty&#8217;s tcnative library, with and without loom support, building both the fat jar and native binary, etc.
Depending on the combinations, the dependencies of the project may be a little different, or the build configuration may be a little different.</p>
</div>
<div class="paragraph">
<p>It was an interesting challenge to pick up and the <a href="https://github.com/yawkat/micronaut-http-benchmarks">solution turned out to be quite elegant</a> and very powerful.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_conceptual_design">Conceptual design</h2>
<div class="sectionbody">
<div class="paragraph">
<p>I have tried several options before this one, which I&#8217;m going to explain below, but let&#8217;s focus with the <em>final design</em> (at least at the moment I write this blog post).
The matrix of artifacts to be generated can be configured in the <code>settings.gradle</code> file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="groovy">combinations {
    dimension("tcnative") {   <i class="conum" data-value="1"></i><b>(1)</b>
        variant("off")
        variant("on")
    }
    dimension("epoll") {      <i class="conum" data-value="2"></i><b>(2)</b>
        variant("off")
        variant("on")
    }
    dimension("json") {       <i class="conum" data-value="3"></i><b>(3)</b>
        variant("jackson")
        variant("serde")
    }
    dimension("micronaut") {  <i class="conum" data-value="4"></i><b>(4)</b>
        variant("3.8")
        variant("4.0")
    }
    dimension("java") {       <i class="conum" data-value="5"></i><b>(5)</b>
        variant("11")
        variant("17")
    }
    exclude {                 <i class="conum" data-value="6"></i><b>(6)</b>
        // Combination of Micronaut 4 and Java 11 is invalid
        it.contains("micronaut-4.0") &amp;&amp; it.contains("java-11")
    }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>a dimension called <code>tcnative</code> is defined with 2 variants, <code>on</code> and <code>off</code></td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>another dimension called <code>epool</code> also has <code>on</code> and <code>off</code> variants</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>the <code>json</code> dimension will let us choose 2 different serialization frameworks: Jackson or Micronaut Serde</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>we can also select the version of Micronaut we want to test</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>as well as the Java version!</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>some invalid combinations can be excluded</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The generates a number of <em>synthetic Gradle projects</em>, that is to say "projects" in the Gradle terminology, but without actually duplicating sources and directories on disk.
With the example above, we generate the following projects:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>:test-case:tcnative-off:epoll-off:json-jackson:micronaut-3.8:java-11</p>
</li>
<li>
<p>:test-case:tcnative-off:epoll-off:json-jackson:micronaut-3.8:java-17</p>
</li>
<li>
<p>:test-case:tcnative-off:epoll-off:json-jackson:micronaut-4.0:java-17</p>
</li>
<li>
<p>:test-case:tcnative-off:epoll-off:json-serde:micronaut-3.8:java-11</p>
</li>
<li>
<p>:test-case:tcnative-off:epoll-off:json-serde:micronaut-3.8:java-17</p>
</li>
<li>
<p>:test-case:tcnative-off:epoll-off:json-serde:micronaut-4.0:java-17</p>
</li>
<li>
<p>:test-case:tcnative-off:epoll-on:json-jackson:micronaut-3.8:java-11</p>
</li>
<li>
<p>:test-case:tcnative-off:epoll-on:json-jackson:micronaut-3.8:java-17</p>
</li>
<li>
<p>:test-case:tcnative-off:epoll-on:json-jackson:micronaut-4.0:java-17</p>
</li>
<li>
<p>&#8230;&#8203; and more</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To build the fat jar of the "tcnative on", "epoll on", "Jackson", "Micronaut 4.0" on Java 17 combination, you can invoke:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="bash">$ ./gradlew :test-case:tcnative-on:epoll-on:json-jackson:micronaut-4.0:java-17:shadowJar</code></pre>
</div>
</div>
<div class="paragraph">
<p>And building the native image of the "tcnative off", "epoll on", "Micronaut Serde", "Micronaut 3.8" on Java 17 combination can be done with:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="bash">$ ./gradlew :test-case:tcnative-off:epoll-on:json-serde:micronaut-3.8:java-17:nativeCompile</code></pre>
</div>
</div>
<div class="paragraph">
<p>Cherry on the cake, all variants can be built in parallel by executing either <code>./gradlew shadowJar</code> (for the fat jars) or <code>./gradlew nativeCompile</code> (for the native binaries), which would copy all the artifacts under the root projects <code>build</code> directory so that they are easy to find in a single place.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_how_does_it_work">How does it work?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In a typical project, say the Micronaut application we want to benchmark, you would have a project build which consists of a single Micronaut application module.
For example, running <code>./gradlew build</code> would build that single project artifacts.
In a multi-project build, you could have several modules, for example <code>core</code> and <code>app</code>, and running <code>:core:build</code> would only build the core library and <code>:app:build</code> would build both <code>core</code> and <code>app</code> (assuming <code>app</code> depends on <code>core</code>.
In both cases, single or multi-project builds, for a typical Gradle project, there&#8217;s a <em>real</em> directory associated for each project <code>core</code>, <code>app</code>, etc, where we can find sources, resources, build scripts, etc.</p>
</div>
<div class="paragraph">
<p>For <em>synthetic projects</em>, we actually generate Gradle projects (aka modules) programmatically.
We have a skeleton directory, called <a href="https://github.com/yawkat/micronaut-http-benchmarks/tree/master/test-case-common"><code>test-case-common</code></a>, which actually defines our application sources, configuration files, etc.
It also contains a build script which applies a <a href="https://github.com/yawkat/micronaut-http-benchmarks/blob/master/build-logic/src/main/kotlin/io.micronaut.testcase.gradle.kts">single convention plugin</a>, named <code>io.micronaut.testcase</code>.
This plugin basically corresponds to our "baseline" build: it applies the Micronaut plugin, adds a number of dependencies, configures native image building, etc.</p>
</div>
<div class="paragraph">
<p>Then the "magic" is to use Gradle&#8217;s <a href="https://melix.github.io/blog/2021/12/composition-in-gradle.html">composition model</a> for the variant aspects.
For example, when we define the <code>tcnative</code> dimension with 2 variants <code>on</code> and <code>off</code>, we&#8217;re modeling the fact that there are 2 possible outcomes for this dimension.
In practice, enabling <code>tcnative</code> is just a matter of adding a single dependency at runtime:</p>
</div>
<div class="listingblock">
<div class="title">io.micronaut.testcase.tcnative.on.gradle.kts</div>
<div class="content">
<pre class="pygments highlight"><code data-lang="kotlin">dependencies {
    runtimeOnly("io.netty:netty-tcnative-boringssl-static::linux-x86_64")
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The dimension which handles the version of Java (both to compile and run the application) makes use of Gradle&#8217;s toolchain support:</p>
</div>
<div class="listingblock">
<div class="title">io.micronaut.testcase.java.17.gradle.kts</div>
<div class="content">
<pre class="pygments highlight"><code data-lang="kotlin">java {
    toolchain {
        languageVersion.set(JavaLanguageVersion.of(17))
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>This can be done in a <em>convention plugin</em> which is named against the dimension variant name: <code>io.micronaut.testcase.tcnative.on</code>.
In other words, the project with path <code>:test-case:tcnative-off:epoll-off:json-jackson:micronaut-3.8:java-11</code> will have a "synthetic" build script which only consists of applying the following plugins:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="groovy">plugins {
    id("io.micronaut.testcase")               <i class="conum" data-value="1"></i><b>(1)</b>
    id("io.micronaut.testcase.tcnative.off")  <i class="conum" data-value="2"></i><b>(2)</b>
    id("io.micronaut.testcase.epoll.off")     <i class="conum" data-value="3"></i><b>(3)</b>
    id("io.micronaut.testcase.json.jackson")  <i class="conum" data-value="4"></i><b>(4)</b>
    id("io.micronaut.testcase.micronaut.3.8") <i class="conum" data-value="5"></i><b>(5)</b>
    id("io.micronaut.testcase.java.11")       <i class="conum" data-value="6"></i><b>(6)</b>
}</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Applies the common configuration</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Configures <code>tcnative</code> off</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Configures <code>epoll</code> off</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Configures Jackson as the serialization framework</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Configures Micronaut 3.8</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>Configures build for Java 11</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Each of these plugins can be found in our <a href="https://github.com/yawkat/micronaut-http-benchmarks/tree/master/build-logic/src/main/kotlin">build logic</a>.
As you can see when browsing the build logic directory, there is actually one small optimization: it is not necessary to create a variant script if there&#8217;s nothign to do.
For example, in practice, <code>tcnative</code> off doesn&#8217;t need any extra configuration, so there&#8217;s no need to write a <code>io.micronaut.testcase.tcnative.off</code> plugin which would be empty in any case.</p>
</div>
<div class="sect3">
<h4 id="_variant_specific_code">Variant specific code</h4>
<div class="paragraph">
<p>The best case would have been that we only have to tweak the build process (for example to add dependencies, disable native image building, etc), but in some cases, we have to change the actual sources or resource files.
Again, we leveraged Gradle&#8217;s flexibility to define custom conventions in our project layout.
In a traditional Gradle (or Maven) project, the main sources are found in <code>src/main/java</code>.
This is the case here, but we also support adding source directories based on the variants.
For example in this project, some DTOs will make use of Java records on Java 17, but those are not available in Java 11, so we need to write 2 variants of the same classes: one with records, the other one with good old Java beans.
This can be done by putting the Java 11 sources under <code>src/main/<strong>variants</strong>/java-11/java</code>, and their equivalent Java 17 sources under <code>src/main/<strong>variants</strong>/java-17/java</code>.
This is actually generic: you can use any variant name in place of <code>java-11</code>: we <em>could</em>, for example, have a source directory for the <code>epoll-on</code> folder.
The same behavior is available for resources (in <code>src/main/<strong>variants</strong>/java-11/resources</code>).</p>
</div>
<div class="paragraph">
<p>This provides very good flexibility while being totally understandable and conventional.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_settings_plugin">The settings plugin</h4>
<div class="paragraph">
<p>So far, we explained how a user interacts with this build, for example by adding a dimension and a variant or adding specific sources, but we didn&#8217;t explain how the projects are actually generated.
For this purpose, we have to explain that Gradle supports multiple types of plugins.
The typical plugins, which we have used so far in this blog post, the <code>io.micronaut.testcase.xxx</code> plugins, are <em>project plugins</em>, because they apply on the <code>Project</code> of a Gradle build.
There are other types of plugins, and the other one which we&#8217;re interested in here is the settings plugin.
Unlike project plugins, these plugins are applied on the <code>Settings</code> object, that is to say thay they would be typically applied on the <code>settings.gradle(.kts</code>) file.
This is what we have in this project:</p>
</div>
<div class="listingblock">
<div class="title">settings.gradle.kts</div>
<div class="content">
<pre class="pygments highlight"><code data-lang="kotlin">// ...

plugins {
    id("io.micronaut.bench.variants")
}


include("load-generator-gatling")

configure&lt;io.micronaut.bench.AppVariants&gt; {
    combinations {
        dimension("tcnative") {
            variant("off")
            variant("on")
        }
        dimension("epoll") {
            variant("off")
            variant("on")
        }
        dimension("json") {
            variant("jackson")
            //variant("serde")
        }
        dimension("micronaut") {
            variant("3.8")
            //variant("4.0")
        }
        dimension("java") {
            //variant("11")
            variant("17")
        }
        exclude {
            // Combination of Micronaut 4 and Java 11 is invalid
            it.contains("micronaut-4.0") &amp;&amp; it.contains("java-11")
        }
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>io.micronaut.bench.variants</code> is another convention plugin <a href="https://github.com/yawkat/micronaut-http-benchmarks/blob/master/build-logic/src/main/kotlin/io.micronaut.bench.variants.settings.gradle.kts">defined in our build logic</a>.
It doesn&#8217;t do much, except for creating an <em>extension</em>, which is what lets us configure the variants:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="kotlin">import io.micronaut.bench.AppVariants

val variants = extensions.create&lt;AppVariants&gt;("benchmarkVariants", settings)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The logic actually happens within that <code>AppVariants</code> class, for which <a href="https://github.com/yawkat/micronaut-http-benchmarks/blob/master/build-logic/src/main/kotlin/io/micronaut/bench/AppVariants.kt">you can find the sources here</a>.
This class handles both the <code>variants</code> extension DSL and the logic to generate the projects.</p>
</div>
<div class="paragraph">
<p>The entry point is the <code>combinations</code> method which takes a configuration block.
Each of the call to <code>dimension</code> registers a new dimension, which is itself configured via a variant configuration block, where each individual variant is declared.
When we return from this call, we have built a model of dimension of variants, for which we need to compute the cartesian product.</p>
</div>
<div class="paragraph">
<p>We can check each of the entry that we have generated against the excludes, and if the combination is valid, we can use the Gradle APIs which are available in settings script to generate our synthetic projects.</p>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="kotlin">val projectPath = ":test-case:${path.replace('/', ':')}"
settings.include(projectPath)</code></pre>
</div>
</div>
<div class="paragraph">
<p>computes the project path (with colons) and includes it, which is equivalent to writing this manually in the <code>settings.gradle</code> file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="groovy">include(":test-case:tcnative-off:epoll-off:json-jackson:micronaut-3.8:java-11")
include(":test-case:tcnative-off:epoll-off:json-jackson:micronaut-3.8:java-17")
include(":test-case:tcnative-off:epoll-off:json-jackson:micronaut-4.0:java-17")</code></pre>
</div>
</div>
<div class="paragraph">
<p>If we stopped here, then we would have defined projects, but Gradle would expect the sources and build scripts for these projects to be found in <code>test-case/tcnative-off/epoll-off/json-jackson/micronaut-3.8/java-11</code>.
This isn&#8217;t the case for us, since all projects will share the same project directory (<code>test-case-common</code>).
However, if we configure all the projects to use the same directory, then things could go wrong at build time, in particular because we use parallel builds: all the projects would write their outputs in the same <code>build</code> directory, but as we have seen, they may have different sources, different dependencies, etc.
So we need to set both the project directory to the common directory, but <em>also</em> change the build directory to a per-project specific directory.
This way we make sure to reuse the same sources without having to copy everything manually, but we also make sure that up-to-date checking, build caching and parallel builds work perfectly fine:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="kotlin">settings.project(projectPath).setProjectDir(File(settings.rootDir, "test-case-common"))
gradle.beforeProject {
    if (this.path == projectPath) {
        setBuildDir(File(projectDir, "build/${path}"))
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that we have to use the <code>gradle.beforeProject</code> API for this: it basically provides us with the naked <code>Project</code> instance of our synthetic projects, before its configuration phase is triggered.</p>
</div>
<div class="paragraph">
<p>The next step is to make sure that once the <code>java</code> plugin is applied on a project, we configure the additional source directories for each dimension.
This is done via the <code>withPlugin</code> API which lets use react on the application of a plugin, and the <code>SourceSet</code> API:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="kotlin">project.plugins.withId("java") {
    project.extensions.findByType(JavaPluginExtension::class.java)?.let { java -&gt;
        variantNames.forEach { variantName -&gt;
            java.sourceSets.all {
                this.java.srcDir("src/$name/variants/$variantName/java")
                this.resources.srcDir("src/$name/variants/$variantName/resources")
            }
        }
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Last, we need to apply our convention plugins, the plugins which correspond to a specific combination variant, to our synthetic project:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="kotlin">gradle.afterProject {
    if (this.path == projectPath) {
        variantSpecs.forEach {
            val pluginId = "io.micronaut.testcase.${it.dimensionName}.${it.name}"
            val plugin = File(settings.settingsDir, "build-logic/src/main/kotlin/$pluginId.gradle.kts")
            if (plugin.exists()) {
                plugins.apply(pluginId)
            }
        }
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>As you can see, for each variant, we basically compute the name of the plugin to apply, and if a corresponding file exists, we simply apply the plugin, that&#8217;s it!</p>
</div>
<div class="paragraph">
<p>It only takes around 100 lines of code to implement both the DSL and logic to generate all this, which is all the power Gradle gives us!</p>
</div>
</div>
<div class="sect3">
<h4 id="_limitations">Limitations</h4>
<div class="paragraph">
<p>Of course, there are limitations to this approach. While we could handle the Java version easily, we can&#8217;t, however, add a dimension we would have needed : GraalVM CE vs GraalVM EE.
This is a <a href="https://github.com/gradle/gradle/pull/18028">limitation of Gradle&#8217;s toolchain support</a>, which cannot make a difference between those 2 toolchains.</p>
</div>
<div class="paragraph">
<p>Another limitation is that this works well for a <em>single project build</em>, or a project like here where there&#8217;s a common application, a support library, but all modifications happen in a single project (the application).
Supporting multi-project builds and variants <em>per module</em> would be possible in theory, but would add quite a lot of complexity.</p>
</div>
<div class="paragraph">
<p>It was also lucky that I could support both Micronaut 3 and Micronaut 4: in practice, the Gradle plugin for Micronaut 4 isn&#8217;t compatible with Micronaut 3, so I would have to either use Micronaut 3 or Micronaut 4.
However, we can use the Micronaut 4 plugin with Micronaut 3, provided <a href="https://github.com/yawkat/micronaut-http-benchmarks/blob/master/build-logic/src/main/kotlin/io.micronaut.testcase.micronaut.3.8.gradle.kts#L9-L14">some small tweaks</a>.</p>
</div>
<div class="paragraph">
<p>Last, there is one unknown to this, which is that building synthetic projects like that makes use of APIs which are stable in Gradle, but likely to be deprecated in the future (event based APIs).</p>
</div>
</div>
<div class="sect3">
<h4 id="_alternatives">Alternatives</h4>
<div class="paragraph">
<p>Before going to the "final" solution, I have actually tried a few things (which could be spiked in a couple hours or so).
In particular, the first thing I did was actually to use a <em>single project</em>, but configure additional artifacts (e.g jar and native binary) for each variant.
While I could make it work, the implementation turned out to be more complicated, because you have to understand how each of the plugins work (Micronaut, GraalVM, the Shadow plugin) and create exotic tasks to make things work.
Also this had a number of drawbacks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>impossible to build variants in parallel (at least without the experimental configuration cache)</p>
</li>
<li>
<p>configuring each of the variant specific build configuration (e.g adding dependencies) was more complicated. It was in particular only possible to add additional <em>runtime</em> dependencies. If something else was needed, for example compile time dependencies or additional resources, this wasn&#8217;t possible to do because a <em>single</em> main jar was produced.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_conclusion">Conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this blog post, we have seen how we can leverage Gradle&#8217;s flexibility to support what seemed to be a complicated use case: given a common codebase and some "small tweaks", generate a matrix of builds which are used to build different artifacts, in order to benchmark them.</p>
</div>
<div class="paragraph">
<p>The solution turned out to be quite simple to implement, and I hope pretty elegant, both in terms of user facing features (adding dimensions and configuring the build should be easy), maintenance (composition over inheritance makes it very simple to understand how things are combined) and implementation.</p>
</div>
<div class="paragraph">
<p>Many thanks to <a href="https://infosec.exchange/@yawkat">Jonas Konrad</a> for the feature requests and for reviewing this blog post!</p>
</div>
</div>
</div></p>
	
	<hr />
	
	<p>Older posts are available in the <a href="/blog/archive.html">archive</a>.</p>

		</div>
		<div id="push"></div>
    </div>
    
    <div id="footer">
      <div class="container">
	<p class="muted credit">All posts on this blog are published with a <em>Creative Commons by-nc-sa</em> license.<a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/2.0/fr/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/2.0/fr/88x31.png"/></a></p>
        <p class="muted credit">Baked with <a href="https://jbake.org">JBake v2.6.6</a></p>
      </div>
    </div>
    
    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    
    <script src="/blog/js/jquery-1.11.1.min.js"></script>
    <script src="/blog/js/bootstrap.min.js"></script>
   <script>hljs.highlightAll();</script>
  </body>
</html>
